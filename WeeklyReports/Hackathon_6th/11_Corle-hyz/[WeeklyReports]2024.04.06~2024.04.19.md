### 姓名
何咏哲

### 实习项目
全自动并行架构升级

### 本周工作

1. **了解Recompute，阅读论文，学习Paddle的Recompute实现**

   * Recompute的原理：在LLM的训练中，forward会从第一层往最后一层计算，然后backward再根据计算的结果从最后一层往第一层计算相应的梯度、使得loss向下降最快的方向变化。在这个过程中，前向计算得到的激活显存需要为反向的计算暂存。但在LLM的训练中，显存已经成为了制约大模型发展的首要因素，因此，Recompute技术就是将前向计算得到的激活显存暂时释放，等到反向计算其梯度时再重新计算，从而达到减小显存开销的目的。
  
   * Recompute的使用：为了使重计算达到最大的收益，我们应该选择计算量小、显存占用大的部分进行重计算，比较符合这一要求的是Attention层，特别是Core Attn部分。因此，重计算也分为不同的粒度，目前主要是core_attn, full_attn, full三种粒度
  
   * 并行策略：不同的并行策略会带来不一样的显存开销，这同样影响着recompute的收益。例如在不开启重计算时，每一个transformer layer的显存开销为sbh(34+5as/h)，在tp+sp时，变成了sbh(34/t+5as/th)
  
   * Recompute与并行模式：需要考虑到重计算的粒度、以及各种并行模式对显存的影响。理论上，完全的重计算可以使得GPU仅需要留存一个激活显存的结果，即2sbh大小的空间（开启amp，每个激活张量占2 bytes）



### 下周工作

1. 在现有的同构的全自动并行方案的基础上，通过自定义一些经验规则、借助显存公式进行负载均衡、动态调整与策略剪枝，构建一个异构体系下的全自动并行方案。

### 导师点评
本周深入地了解了Recompute技术，对其具体步骤有自己的理解，下一步可开展异构体系下全自动并行方案的研究。
